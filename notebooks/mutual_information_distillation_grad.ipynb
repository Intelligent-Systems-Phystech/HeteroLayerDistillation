{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T22:20:57.789633Z",
     "start_time": "2022-04-25T22:20:55.750311Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code')\n",
    "from resnet import *\n",
    "# from funcs import *\n",
    "from cifar_very_tiny import *\n",
    "from cifar_tiny import *\n",
    "from cifar_dataset import *    \n",
    "import torch as t \n",
    "import numpy as np\n",
    "from numpy import polyfit\n",
    "from numpy import polyval\n",
    "import tqdm\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.cm as cm\n",
    "import json\n",
    "# import hyperparams\n",
    "from importlib import reload\n",
    "from scipy.interpolate import interp1d\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize']=(12,9)\n",
    "plt.rcParams['font.size']= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T22:21:00.802489Z",
     "start_time": "2022-04-25T22:21:00.798491Z"
    }
   },
   "outputs": [],
   "source": [
    "epoch_num = 25\n",
    "# epoch_num = 50\n",
    "\n",
    "run_num = 2 # количество запусков эксперимента\n",
    "\n",
    "# версия нужна, чтобы различать старые и новые результаты экспериментов. \n",
    "# менять нужно каждый раз, когда есть хотя бы незначительные изменения в эксперименте\n",
    "experiment_version = '3'\n",
    "\n",
    "validate_every_epoch = 5 \n",
    "\n",
    "# train_splines_every_epoch = 5 # каждые 5 эпох отслеживать траекторию гиперпараметров\n",
    "# train_splines_every_epoch = 2\n",
    "# train_splines_every_epoch = 3\n",
    "train_splines_every_epoch = 10\n",
    "\n",
    "# размер мини-эпохи в батчах, за которую у нас производится либо обучение спайлов, либо их использование\n",
    "mini_epoch_size = 10\n",
    "\n",
    "start_beta = 0.5\n",
    "start_temp  = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T22:21:04.204382Z",
     "start_time": "2022-04-25T22:21:02.711435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader_no_augumentation, valid_loader, test_loader = cifar10_loader(batch_size=128, split_train_val=True,\n",
    "                                                                             maxsize=128*100, use_aug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T22:21:20.346094Z",
     "start_time": "2022-04-25T22:21:20.344061Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if t.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T22:21:22.542328Z",
     "start_time": "2022-04-25T22:21:22.524514Z"
    }
   },
   "outputs": [],
   "source": [
    "# This code is a modiciation of https://github.com/khanrc/pt.darts/blob/master/architect.py\n",
    "import copy\n",
    "import torch\n",
    "import math\n",
    "from torch.optim import adam\n",
    "\n",
    "\n",
    "class AdamHyperGradCalculator():\n",
    "    \"\"\" Compute gradients of hyperparameters wrt parameters are optimizaed by Adam \"\"\"\n",
    "    def __init__(self,  net, parameters_loss_function, hyperparameters_loss_function, optimizer, h, additional_params):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            net\n",
    "            w_momentum: weights momentum\n",
    "        \"\"\"\n",
    "        self.net = net\n",
    "        self.v_net =  None # lazy\n",
    "        self.mu_feat, self.log_sigma_feat = additional_params\n",
    "        self.w = list(self.net.parameters()) + list(self.mu_feat.parameters()) + [self.log_sigma_feat]\n",
    "        self.w_loss = parameters_loss_function #data,model, h\n",
    "        self.h_loss = hyperparameters_loss_function #x,y,model\n",
    "        self.optimizer = optimizer\n",
    "        self.h = list(h)\n",
    "        \n",
    "\n",
    "\n",
    "    def virtual_step(self, trn):\n",
    "        \"\"\"\n",
    "        Compute unrolled weight w' (virtual step)\n",
    "        Step process:\n",
    "        1) forward\n",
    "        2) calc loss\n",
    "        3) compute gradient (by backprop)\n",
    "        4) update gradient\n",
    "        \"\"\"\n",
    "        # forward & calc loss\n",
    "        lr = self.optimizer.param_groups[0]['lr']\n",
    "        h = self.h \n",
    "        optimizer = self.optimizer\n",
    "        \n",
    "        loss = self.w_loss(trn, self.net, h) # L_trn(w)\n",
    "\n",
    "        # compute gradient\n",
    "        gradients = torch.autograd.grad(loss, list(self.net.parameters()) + list(self.mu_feat.parameters()) + [self.log_sigma_feat], allow_unused=True)\n",
    "        # do virtual step (update gradient)\n",
    "        # below operations do not need gradient tracking\n",
    "        with torch.no_grad():\n",
    "            # dict key is not the value, but the pointer. So original network weight have to\n",
    "            # be iterated also.\n",
    "            for w, vw, g in zip(list(self.net.parameters()) + list(self.mu_feat.parameters()) + [self.log_sigma_feat], list(self.v_net.parameters())  + list(self.mu_feat.parameters()) + [self.log_sigma_feat], gradients):           \n",
    "                #state = optimizer.state[w]\n",
    "                \n",
    "                # Lazy state initialization: not ready yet                    \n",
    "                #if len(state) == 0:\n",
    "                #    return \n",
    "                \"\"\"vw_ = w.clone() \n",
    "                adam([vw_],\n",
    "                             [g],\n",
    "                             [state['exp_avg'].clone()],\n",
    "                             [state['exp_avg_sq'].clone()],  \n",
    "                             None,                           \n",
    "                             [state['step']+1],\n",
    "                             amsgrad = False, \n",
    "                             weight_decay = 0.0,\n",
    "                             beta1 = optimizer.param_groups[0]['betas'][0],\n",
    "                             beta2 = optimizer.param_groups[0]['betas'][1],\n",
    "                             lr = optimizer.param_groups[0]['lr'],\n",
    "                             eps = optimizer.param_groups[0]['eps'])\n",
    "                                                                               \n",
    "                vw.copy_(vw_)\n",
    "                \"\"\"\n",
    "               \n",
    "                vw.copy_(w - optimizer.defaults['lr'] * g)\n",
    "            \n",
    "            \n",
    "    def calc_gradients(self, trn, val):\n",
    "        \"\"\" Compute unrolled loss and backward its gradients\n",
    "        Args:\n",
    "            xi: learning rate for virtual gradient step (same as net lr)\n",
    "            w_optim: weights optimizer - for virtual step\n",
    "        \"\"\"\n",
    "        lr = self.optimizer.param_groups[0]['lr']\n",
    "        h = self.h \n",
    "        optimizer = self.optimizer\n",
    "        #for w in self.net.parameters():\n",
    "        #        state = optimizer.state[w]\n",
    "        #        if len(state)==0:\n",
    "        #            print ('not ready')\n",
    "        #            return\n",
    "                \n",
    "        if self.v_net is None:\n",
    "        \n",
    "            self.v_net = copy.deepcopy(self.net)        \n",
    "        # do virtual step (calc w`)\n",
    "        self.virtual_step(trn)\n",
    "\n",
    "        # calc unrolled loss\n",
    "        loss = self.h_loss(val, self.v_net) # L_val(w`)\n",
    "           \n",
    "        v_grads = torch.autograd.grad(loss,list(self.v_net.parameters()) + list(self.mu_feat.parameters()) + [self.log_sigma_feat])\n",
    "        dw = v_grads\n",
    "\n",
    "        hessian = self.compute_hessian(dw, trn)\n",
    "\n",
    "        # update final gradient = dalpha - xi*hessian\n",
    "        with torch.no_grad():\n",
    "            for alpha,  he in zip(h,  hessian):\n",
    "                alpha.grad =  -lr*he\n",
    "\n",
    "    def compute_hessian(self, dw, trn):\n",
    "        \"\"\"\n",
    "        dw = dw` { L_val(w`, alpha) }\n",
    "        w+ = w + eps * dw\n",
    "        w- = w - eps * dw\n",
    "        hessian = (dalpha { L_trn(w+, alpha) } - dalpha { L_trn(w-, alpha) }) / (2*eps)\n",
    "        eps = 0.01 / ||dw||\n",
    "        \"\"\"\n",
    "        h = self.h\n",
    "        norm = torch.cat([w.view(-1) for w in dw]).norm()\n",
    "\n",
    "        eps = 1e-2 / norm\n",
    "      \n",
    "\n",
    "        # w+ = w + eps*dw`\n",
    "        with torch.no_grad():\n",
    "            for p, d in zip(list(self.net.parameters()) + list(self.mu_feat.parameters()) + [self.log_sigma_feat], dw):\n",
    "                p += eps * d\n",
    "        loss = self.w_loss(trn, self.net, h)\n",
    "        dalpha_pos = torch.autograd.grad(loss, h) # dalpha { L_trn(w+) }\n",
    "\n",
    "        # w- = w - eps*dw`\n",
    "        with torch.no_grad():\n",
    "            for p, d in zip(self.net.parameters(), dw):\n",
    "                p -= 2. * eps * d\n",
    "        loss = self.w_loss(trn, self.net, h)\n",
    "        dalpha_neg = torch.autograd.grad(loss, h) # dalpha { L_trn(w-) }\n",
    "\n",
    "        # recover w\n",
    "        with torch.no_grad():\n",
    "            for p, d in zip(self.net.parameters(), dw):\n",
    "                p += eps * d\n",
    "\n",
    "        hessian = [(p-n) / (2.*eps) for p, n in zip(dalpha_pos, dalpha_neg)]\n",
    "        return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T22:21:25.969608Z",
     "start_time": "2022-04-25T22:21:25.947394Z"
    }
   },
   "outputs": [],
   "source": [
    "# определяем функцию потерь как замкнутую относительно аргументов функцию\n",
    "# нужно для подсчета градиентов гиперпараметров по двухуровневой оптимизации\n",
    "def param_loss_mi(batch,model,h):\n",
    "    lam1 = h[0]\n",
    "    x,y,teacher_feat,mu_feat,log_sigma_feat = batch    \n",
    "    student_feat, student_logits = model.get_features(x, [2,3])\n",
    "    class_loss = crit(student_logits, y)\n",
    "    sigma2 = torch.log(1+torch.exp(log_sigma_feat))\n",
    "    feat_loss = ((mu_feat(teacher_feat) - student_feat)**2).sum(1).mean()/(2*sigma2) + 0.5*torch.log(sigma2)*np.prod(teacher_feat.shape[1:])\n",
    "    loss = class_loss * (1.0-lam1) + feat_loss * lam1\n",
    "    return loss\n",
    "\n",
    "# определяем функцию валидационную функцию потерь как замкнутую относительно аргументов функцию\n",
    "# нужно для подсчета градиентов гиперпараметров по двухуровневой оптимизации\n",
    "def hyperparam_loss_mi(batch, model):\n",
    "    x,y = batch\n",
    "    student_feat, student_logits = model.get_features(x, [2,3])\n",
    "    class_loss = crit(student_logits, y)            \n",
    "    return class_loss\n",
    "\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "def dist_with_opt(experiment_version, train_loader_no_augumentation, test_loader, validation_loader, validate_every_epoch, lambdas=None, clip_grad=10e-3, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    t.manual_seed(seed)\n",
    "    \n",
    "    # for lam1 in [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 0.5]:\n",
    "    lam1 = t.nn.Parameter(t.tensor(np.random.uniform(low=0.0, high=1.0), device=device), requires_grad=True)\n",
    "\n",
    "    if lambdas is not None: # non-random initialization\n",
    "        lam1.data *= 0\n",
    "        lam1.data += lambdas[0]\n",
    "\n",
    "    student = Cifar_Very_Tiny(10).cpu()\n",
    "    teacher = Cifar_Tiny(10).cpu() \n",
    "    teacher.load_state_dict(torch.load('tiny_cifar10.model?raw=true', map_location=torch.device('cpu')))\n",
    "    #scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=10, gamma=0.5)   \n",
    "    mu_feat = nn.Linear(128, 128).cpu()\n",
    "    log_sigma_feat = torch.nn.Parameter(torch.zeros(1).cpu())\n",
    "\n",
    "    #mu_logit = nn.Linear(10, 10).cpu()\n",
    "    #log_sigma_logit = torch.nn.Parameter(torch.zeros(1).cpu())\n",
    "    h =[lam1]\n",
    "\n",
    "    optim = torch.optim.Adam(list(student.parameters()) + list(mu_feat.parameters()) + [log_sigma_feat])    \n",
    "    optim2 = torch.optim.Adam(h)    \n",
    "    hyper_grad_calc = AdamHyperGradCalculator(student, param_loss_mi, hyperparam_loss_mi, optim, h, [mu_feat, log_sigma_feat])\n",
    "    val_load_iter = iter(validation_loader)\n",
    "\n",
    "    for e in range(25):\n",
    "        tq = tqdm.tqdm(train_loader_no_augumentation)\n",
    "        losses = []\n",
    "\n",
    "        for batch_id, (x,y) in enumerate(tq):\n",
    "            try:\n",
    "                 (v_x, v_y) = next(val_load_iter)\n",
    "            except:                    \n",
    "                val_load_iter = iter(val_load)\n",
    "                (v_x, v_y) = next(val_load_iter)\n",
    "\n",
    "                \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            v_x = v_x.to(device)\n",
    "            v_y = v_y.to(device)\n",
    "            optim2.zero_grad()\n",
    "            teacher_feat, teacher_logits = teacher.get_features(x, [2,3])\n",
    "            hyper_grad_calc.calc_gradients((x,y,teacher_logits,mu_feat,log_sigma_feat), (v_x, v_y))                    \n",
    "            t.nn.utils.clip_grad_value_(h, clip_grad)\n",
    "\n",
    "            for h_ in h:\n",
    "                if h_.grad is not None:\n",
    "                    h_.grad = t.where(t.isnan(h_.grad), t.zeros_like(h_.grad), h_.grad)\n",
    "            \n",
    "            optim2.step()\n",
    "            if lam1 > 1.0:\n",
    "                lam1.data*=0.0\n",
    "                lam1.data+=1.0\n",
    "            if lam1 < 0.0:\n",
    "                lam1.data*=0.0\n",
    "                   \n",
    "            optim.zero_grad()\n",
    "            student_feat, student_logits = student.get_features(x, [2,3])\n",
    "            # class_loss = crit(student_logits, y)\n",
    "            # sigma2 = torch.log(1+torch.exp(log_sigma_feat))\n",
    "            # feat_loss = ((mu_feat(teacher_feat) - student_feat)**2).sum(1).mean()/(2*sigma2) + 0.5*torch.log(sigma2)*np.prod(teacher_feat.shape[1:])\n",
    "            #logit_loss =((mu_feat(teacher_feat) - student_feat)**2).sum(1).mean()/(2*sigma2) + 0.5*torch.log(sigma2)*np.prod(teacher_feat.shape[1:])\n",
    "            #lam1 = 0.5\n",
    "            loss = param_loss_mi((x,y,teacher_logits,mu_feat,log_sigma_feat), student,h)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "            tq.set_description('current loss:{}'.format(np.mean(losses[-10:])))      \n",
    "        #scheduler.step()\n",
    "        # student.eval()\n",
    "\n",
    "        if e==0 or (e+1)%validate_every_epoch == 0:\n",
    "            test_loss = []\n",
    "            student.eval()\n",
    "            for x,y in test_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                student_feat, student_logits = student.get_features(x, [2,3])\n",
    "                test_loss.append(crit(student_logits, y).detach().cpu().numpy())\n",
    "            test_loss = float(np.mean(test_loss))\n",
    "            val_loss = []\n",
    "            for x,y in validation_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                student_feat, student_logits = student.get_features(x, [2,3])\n",
    "                val_loss.append(crit(student_logits, y).detach().cpu().numpy())\n",
    "            val_loss = float(np.mean(val_loss))\n",
    "        \n",
    "        ac = float(accuracy(student, test_loader))\n",
    "        student.train()\n",
    "\n",
    "        # if not hyperopt:\n",
    "        internal_results.append({'epoch': e, 'test loss':test_loss, 'val loss':val_loss, 'accuracy':ac,\n",
    "                             'lambda1':float(lam1.cpu().detach().numpy()),\n",
    "                            })\n",
    "        # else:\n",
    "        #     val_acc = float(accuracy(student, validation_loader))\n",
    "        #     internal_results.append({'epoch': e, 'test loss':test_loss, 'val loss':val_loss, 'accuracy':ac,\n",
    "        #                          'lambda1':float(lam1.cpu().detach().numpy()),\n",
    "        #                           'val acc':val_acc})\n",
    "        print (internal_results[-1])\n",
    "\n",
    "    # if not hyperopt: # outer function optimization\n",
    "    with open('../logs/acc_mi_'+experiment_version+'.txt','a') as out:\n",
    "        out.write(json.dumps({'results':internal_results, 'version': exp_ver})+'\\n')\n",
    "    # else:\n",
    "    #     # inner function for hyperopt optimization\n",
    "    #     return max([res['val acc'] for res in internal_results])\n",
    "        \n",
    "        # with open('../logs/acc_mi_'+experiment_version+'.txt','a') as out:\n",
    "        #     out.write('{}: {}: {}\\n'.format(lam1, e, ac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T22:21:27.709909Z",
     "start_time": "2022-04-25T22:21:27.308140Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/90 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4f7a1fd57fbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdist_with_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_no_augumentation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_every_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-15a0956a5e16>\u001b[0m in \u001b[0;36mdist_with_opt\u001b[0;34m(experiment_version, train_loader_no_augumentation, test_loader, validation_loader, validate_every_epoch, lambdas, clip_grad, seed)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0moptim2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mteacher_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mhyper_grad_calc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mteacher_logits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu_feat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_sigma_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_value_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-08e3343d6283>\u001b[0m in \u001b[0;36mcalc_gradients\u001b[0;34m(self, trn, val)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# do virtual step (calc w`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvirtual_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# calc unrolled loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-08e3343d6283>\u001b[0m in \u001b[0;36mvirtual_step\u001b[0;34m(self, trn)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mvw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "dist_with_opt(experiment_version, train_loader_no_augumentation, test_loader, valid_loader, validate_every_epoch, lambdas=[1e-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_with_no_opt(experiment_version, train_loader_no_augumentation, test_loader, validation_loader, validate_every_epoch, lambdas=None, file=True, no_tqdm=False, clip_grad=10e-3, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    t.manual_seed(seed)\n",
    "\n",
    "    lam1 = t.nn.Parameter(t.tensor(np.random.uniform(low=0.0, high=1.0), device=device), requires_grad=True)\n",
    "\n",
    "    if lambdas is not None: # non-random initialization\n",
    "        lam1.data *= 0\n",
    "        lam1.data += lambdas[0]\n",
    "    \n",
    "    student = Cifar_Very_Tiny(10).cpu()\n",
    "    teacher = Cifar_Tiny(10).cpu() \n",
    "    teacher.load_state_dict(torch.load('tiny_cifar10.model?raw=true', map_location=torch.device('cpu')))\n",
    "    #scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=10, gamma=0.5)   \n",
    "    mu_feat = nn.Linear(128, 64).cpu()\n",
    "    log_sigma_feat = torch.nn.Parameter(torch.zeros(1).cpu())\n",
    "\n",
    "    #mu_logit = nn.Linear(10, 10).cpu()\n",
    "    #log_sigma_logit = torch.nn.Parameter(torch.zeros(1).cpu())\n",
    "\n",
    "\n",
    "    optim = torch.optim.Adam(list(student.parameters()) + list(mu_feat.parameters()) + [log_sigma_feat])    \n",
    "    val_load_iter = iter(val_load)\n",
    "\n",
    "    for e in range(25):\n",
    "        tq = tqdm.tqdm(train_loader_no_augumentation)\n",
    "        if no_tqdm:\n",
    "            tq = train_loader_no_augumentation\n",
    "        losses = []\n",
    "\n",
    "        for batch_id, (x,y) in enumerate(tq):\n",
    "            try:\n",
    "                 (v_x, v_y) = next(val_load_iter)\n",
    "            except:                    \n",
    "                val_load_iter = iter(val_load)\n",
    "                (v_x, v_y) = next(val_load_iter)\n",
    "                \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optim.zero_grad()\n",
    "            student_feat, student_logits = student.get_features(x, [3,4])\n",
    "            # class_loss = crit(student_logits, y)\n",
    "            # sigma2 = torch.log(1+torch.exp(log_sigma_feat))\n",
    "            # feat_loss = ((mu_feat(teacher_feat) - student_feat)**2).sum(1).mean()/(2*sigma2) + 0.5*torch.log(sigma2)*np.prod(teacher_feat.shape[1:])\n",
    "            #logit_loss =((mu_feat(teacher_feat) - student_feat)**2).sum(1).mean()/(2*sigma2) + 0.5*torch.log(sigma2)*np.prod(teacher_feat.shape[1:])\n",
    "            #lam1 = 0.5\n",
    "            loss = param_loss_mi((x,y,teacher_logits,mu_feat,log_sigma_feat), student,lam1)\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "            if not no_tqdm:\n",
    "                tq.set_description('current loss:{}'.format(np.mean(losses[-10:])))\n",
    "        #scheduler.step()\n",
    "        \n",
    "        if e==0 or (e+1)%validate_every_epoch == 0:\n",
    "            test_loss = []\n",
    "            student.eval()\n",
    "            for x,y in test_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                student_feat, student_logits = student.get_features(x, [3,4])\n",
    "                test_loss.append(crit(student_logits, y).detach().cpu().numpy())\n",
    "            test_loss = float(np.mean(test_loss))\n",
    "            val_loss = []\n",
    "            for x,y in validation_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                student_feat, student_logits = student.get_features(x, [3,4])\n",
    "                val_loss.append(crit(student_logits, y).detach().cpu().numpy())\n",
    "            val_loss = float(np.mean(val_loss))\n",
    "        \n",
    "        ac = float(accuracy(student, test_loader))\n",
    "        \n",
    "        if file:\n",
    "            internal_results.append({'epoch': e, 'test loss':test_loss, 'val loss':val_loss, 'accuracy':ac,\n",
    "                                 'lambda1':float(lam1.cpu().detach().numpy()),\n",
    "                                })\n",
    "        else:\n",
    "            val_acc = float(accuracy(student, validation_loader))\n",
    "            internal_results.append({'epoch': e, 'test loss':test_loss, 'val loss':val_loss, 'accuracy':ac,\n",
    "                                 'lambda1':float(lam1.cpu().detach().numpy()),\n",
    "                                  'val acc':val_acc})\n",
    "        \n",
    "        print (internal_results[-1])\n",
    "\n",
    "    if file: # outer function optimization\n",
    "        with open('../logs/acc_mi_'+experiment_version+'.txt','a') as out:\n",
    "            out.write(json.dumps({'results':internal_results, 'version': experiment_version})+'\\n')\n",
    "    else:\n",
    "        # inner function for hyperopt optimization\n",
    "        return max([res['val acc'] for res in internal_results])\n",
    "        \n",
    "        # with open('../logs/acc_mi_'+experiment_version+'.txt','a') as out:\n",
    "        #     out.write('{}: {}: {}\\n'.format(lam1, e, ac))\n",
    "\n",
    "\n",
    "def dist_hyperopt(experiment_version, run_num, tr_load, t_load, val_load, validate_every_epoch, trial_num):\n",
    "    np.random.seed(42)\n",
    "    t.manual_seed(42)\n",
    "\n",
    "    for _ in range(run_num):\n",
    "        cost_function = lambda lambdas: -dist_with_no_opt(experiment_version, train_loader_no_augumentation, test_loader, validation_loader, validate_every_epoch, lambdas = best_lambdas['lambda1'], file=False, no_tqdm=True) # validation accuracy * (-1) -> min\n",
    "       \n",
    "        best_lambdas = fmin(fn=cost_function,                             \n",
    "        #space= [ hp.uniform('lambda1', 0.0, 1.0), hp.uniform('lambda2', 0.0, 1.0), hp.uniform('temp', 0.1, 10.0)],\n",
    "        space= [ hp.uniform('lambda1', 0.0, 1.0)],  \n",
    "        algo=tpe.suggest,\n",
    "        max_evals=trial_num)\n",
    "        #cifar_with_validation_set(exp_ver, 1, epoch_num, filename, tr_s_epoch, m_e, tr_load, t_load, val_load, validate_every_epoch, lambdas = [best_lambdas['lambda1'], best_lambdas['lambda2'], best_lambdas['temp']],  mode='no-opt')\n",
    "        dist_with_no_opt(experiment_version, train_loader_no_augumentation, test_loader, validation_loader, validate_every_epoch, lambdas = best_lambdas['lambda1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
